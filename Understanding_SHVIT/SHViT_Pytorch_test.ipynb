{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://github.com/maxw1489/Mask_RCNN (tensorflow 2.9.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cgd/anaconda3/envs/shvit/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.layers import SqueezeExcite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "patch_size=16\n",
    "frozen_stages = 0\n",
    "in_chans = 3\n",
    "embed_dim = [224, 336, 448]\n",
    "partial_dim = [48, 72, 96] # partial_dim = r*embed_dim with r=1/4.67\n",
    "qk_dim = [16, 16, 16]\n",
    "depth = [4, 7, 6]\n",
    "types = [\"i\", \"s\", \"s\"]\n",
    "down_ops = [['subsample', 2], ['subsample', 2], ['']]\n",
    "pretrained = None\n",
    "distillation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupNorm(torch.nn.GroupNorm):\n",
    "    \"\"\"\n",
    "    Group Normalization with 1 group.\n",
    "    Input: tensor in shape [B, C, H, W]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, **kwargs):\n",
    "        super().__init__(1, num_channels, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_BN(torch.nn.Sequential):  \n",
    "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
    "                 groups=1, bn_weight_init=1):\n",
    "        super().__init__()\n",
    "        self.add_module('c', torch.nn.Conv2d(\n",
    "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
    "        self.add_module('bn', torch.nn.BatchNorm2d(b))\n",
    "        torch.nn.init.constant_(self.bn.weight, bn_weight_init)\n",
    "        torch.nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        c, bn = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = c.weight * w[:, None, None, None]\n",
    "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
    "            (bn.running_var + bn.eps)**0.5\n",
    "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
    "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BN_Linear(torch.nn.Sequential):\n",
    "    def __init__(self, a, b, bias=True, std=0.02):\n",
    "        super().__init__()\n",
    "        self.add_module('bn', torch.nn.BatchNorm1d(a))\n",
    "        self.add_module('l', torch.nn.Linear(a, b, bias=bias))\n",
    "        trunc_normal_(self.l.weight, std=std)\n",
    "        if bias:\n",
    "            torch.nn.init.constant_(self.l.bias, 0)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        bn, l = self._modules.values()\n",
    "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        b = bn.bias - self.bn.running_mean * \\\n",
    "            self.bn.weight / (bn.running_var + bn.eps)**0.5\n",
    "        w = l.weight * w[None, :]\n",
    "        if l.bias is None:\n",
    "            b = b @ self.l.weight.T\n",
    "        else:\n",
    "            b = (l.weight @ b[:, None]).view(-1) + self.l.bias\n",
    "        m = torch.nn.Linear(w.size(1), w.size(0))\n",
    "        m.weight.data.copy_(w)\n",
    "        m.bias.data.copy_(b)\n",
    "        return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(torch.nn.Module):\n",
    "    def __init__(self, dim, out_dim):\n",
    "        super().__init__()\n",
    "        hid_dim = int(dim * 4)\n",
    "        self.conv1 = Conv2d_BN(dim, hid_dim, 1, 1, 0)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.conv2 = Conv2d_BN(hid_dim, hid_dim, 3, 2, 1, groups=hid_dim)\n",
    "        self.se = SqueezeExcite(hid_dim, .25)\n",
    "        self.conv3 = Conv2d_BN(hid_dim, out_dim, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv3(self.se(self.act(self.conv2(self.act(self.conv1(x))))))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(torch.nn.Module):\n",
    "    def __init__(self, m, drop=0.):\n",
    "        super().__init__()\n",
    "        self.m = m\n",
    "        self.drop = drop\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.drop > 0:\n",
    "            return x + self.m(x) * torch.rand(x.size(0), 1, 1, 1,\n",
    "                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()\n",
    "        else:\n",
    "            return x + self.m(x)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def fuse(self):\n",
    "        if isinstance(self.m, Conv2d_BN):\n",
    "            m = self.m.fuse()\n",
    "            assert(m.groups == m.in_channels)\n",
    "            identity = torch.ones(m.weight.shape[0], m.weight.shape[1], 1, 1)\n",
    "            identity = torch.nn.functional.pad(identity, [1,1,1,1])\n",
    "            m.weight += identity.to(m.weight.device)\n",
    "            return m\n",
    "        else:\n",
    "            return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, ed, h):\n",
    "        super().__init__()\n",
    "        self.pw1 = Conv2d_BN(ed, h)\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.pw2 = Conv2d_BN(h, ed, bn_weight_init=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pw2(self.act(self.pw1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SHSA(torch.nn.Module):\n",
    "    \"\"\"Single-Head Self-Attention\"\"\"\n",
    "    def __init__(self, dim, qk_dim, pdim):\n",
    "        super().__init__()\n",
    "        self.scale = qk_dim ** -0.5\n",
    "        self.qk_dim = qk_dim\n",
    "        self.dim = dim\n",
    "        self.pdim = pdim\n",
    "\n",
    "        self.pre_norm = GroupNorm(pdim)\n",
    "\n",
    "        self.qkv = Conv2d_BN(pdim, qk_dim * 2 + pdim)\n",
    "        self.proj = torch.nn.Sequential(torch.nn.ReLU(), Conv2d_BN(\n",
    "            dim, dim, bn_weight_init = 0))\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x1, x2 = torch.split(x, [self.pdim, self.dim - self.pdim], dim = 1)\n",
    "        x1 = self.pre_norm(x1)\n",
    "        qkv = self.qkv(x1)\n",
    "        q, k, v = qkv.split([self.qk_dim, self.qk_dim, self.pdim], dim = 1)\n",
    "        q, k, v = q.flatten(2), k.flatten(2), v.flatten(2)\n",
    "        \n",
    "        attn = (q.transpose(-2, -1) @ k) * self.scale\n",
    "        attn = attn.softmax(dim = -1)\n",
    "        x1 = (v @ attn.transpose(-2, -1)).reshape(B, self.pdim, H, W)\n",
    "        x = self.proj(torch.cat([x1, x2], dim = 1))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(torch.nn.Module):\n",
    "    def __init__(self, dim, qk_dim, pdim, type):\n",
    "        super().__init__()\n",
    "        if type == \"s\":    # for later stages\n",
    "            self.conv = Residual(Conv2d_BN(dim, dim, 3, 1, 1, groups = dim, bn_weight_init = 0))\n",
    "            self.mixer = Residual(SHSA(dim, qk_dim, pdim))\n",
    "            self.ffn = Residual(FFN(dim, int(dim * 2)))\n",
    "        elif type == \"i\":   # for early stages\n",
    "            self.conv = Residual(Conv2d_BN(dim, dim, 3, 1, 1, groups = dim, bn_weight_init = 0))\n",
    "            self.mixer = torch.nn.Identity()\n",
    "            self.ffn = Residual(FFN(dim, int(dim * 2)))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.ffn(self.mixer(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embed:  torch.Size([1, 224, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.randn(1, 3, 512, 512)\n",
    "\n",
    "patch_embed = torch.nn.Sequential(Conv2d_BN(in_chans, embed_dim[0] // 8, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 8, embed_dim[0] // 4, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 4, embed_dim[0] // 2, 3, 2, 1), torch.nn.ReLU(),\n",
    "                           Conv2d_BN(embed_dim[0] // 2, embed_dim[0], 3, 2, 1)\n",
    "                           )\n",
    "\n",
    "x = patch_embed(input_image)\n",
    "print(\"patch_embed: \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 224 16 48 4 ['subsample', 2] i\n",
      "1 336 16 72 7 ['subsample', 2] s\n",
      "2 448 16 96 6 [''] s\n",
      "4 10 9\n",
      "block1 in :  torch.Size([1, 224, 32, 32])\n",
      "block1 out:  torch.Size([1, 224, 32, 32])\n",
      "block2 in :  torch.Size([1, 224, 32, 32])\n",
      "block2 out:  torch.Size([1, 336, 16, 16])\n",
      "block3 in :  torch.Size([1, 336, 16, 16])\n",
      "block3 out:  torch.Size([1, 448, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "blocks1 = []\n",
    "blocks2 = []\n",
    "blocks3 = []\n",
    "outs = []\n",
    "\n",
    "\n",
    "for i, (ed, kd, pd, dpth, do, t) in enumerate(zip(embed_dim, qk_dim, partial_dim, depth, down_ops, types)):\n",
    "    print (i, ed, kd, pd, dpth, do, t)\n",
    "    for d in range(dpth):\n",
    "        eval('blocks' + str(i+1)).append(BasicBlock(ed, kd, pd, t))\n",
    "    if do[0] == 'subsample':\n",
    "                # Build SHViT downsample block\n",
    "                #('Subsample' stride)\n",
    "                blk = eval('blocks' + str(i+2))\n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i], embed_dim[i], 3, 1, 1, groups=embed_dim[i])),\n",
    "                                    Residual(FFN(embed_dim[i], int(embed_dim[i] * 2))),))\n",
    "                blk.append(PatchMerging(*embed_dim[i:i + 2]))\n",
    "                \n",
    "                blk.append(torch.nn.Sequential(Residual(Conv2d_BN(embed_dim[i + 1], embed_dim[i + 1], 3, 1, 1, groups=embed_dim[i + 1])),\n",
    "                                    Residual(FFN(embed_dim[i + 1], int(embed_dim[i + 1] * 2))),))\n",
    "\n",
    "# print(eval('blocks1'))\n",
    "\n",
    "print(len(blocks1),len(blocks2),len(blocks3)) # 4, 7+3, 6+3\n",
    "\n",
    "blocks1 = torch.nn.Sequential(*blocks1)\n",
    "blocks2 = torch.nn.Sequential(*blocks2)\n",
    "blocks3 = torch.nn.Sequential(*blocks3)\n",
    "\n",
    "print(\"block1 in : \", x.shape)\n",
    "x = blocks1(x)\n",
    "outs.append(x)\n",
    "print(\"block1 out: \", x.shape)\n",
    "print(\"block2 in : \", x.shape)\n",
    "x = blocks2(x)\n",
    "outs.append(x)\n",
    "print(\"block2 out: \", x.shape)\n",
    "print(\"block3 in : \", x.shape)\n",
    "x = blocks3(x)\n",
    "outs.append(x)\n",
    "print(\"block3 out: \", x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 16, 8)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512//16, 512//32, 512//64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([1, 224, 32, 32]), torch.Size([1, 336, 16, 16]), torch.Size([1, 448, 8, 8])]\n"
     ]
    }
   ],
   "source": [
    "shapes = [tensor.shape for tensor in outs]\n",
    "print(shapes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shvit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
